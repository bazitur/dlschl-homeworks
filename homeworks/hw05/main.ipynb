{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 4: нейронная сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Задание основано на ноутбуке курса Школы Анализа Данных по глубинному обучению.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом семинаре мы напишем нейронную сеть с нуля. Реализуемая нами архитектура в целом схожа с тем, как выглядит нейронная сеть в Pytorch.\n",
    "\n",
    "Этот семинар состоит из двух ноутбуков: Modules.ipynb, в котором будет находиться реализация нейронной сети, и этот ноутбук, в котором вы будете выполнять все эксперименты с нейронной сетью. Пожалуйста, не меняйте прототипы функций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейронные сети стали популярными по многим причинам, но одна из них - это модульность. Нейронные сети состоят из модулей (слоев), каждый слой реализует какую-то функциональность. Комбинируя имеющиеся слои можно реализовать state-of-art архитектуру с помощью уже имеющейся библиотеки (Pytorch, Tensorflow итд). Часто для реализации множества прорывных современных идей достаточно определить новый слой, или даже просто слегка изменить уже имеющийся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=img/simple_neural_network_header.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте для начала посмотрим на нейронную сеть как на черный ящик (нас не интересует как он устроен, но когда мы просим его что-то сделать - он вежливо выполняет просьбу). Какую функциональность должна иметь нейронная сеть? Такую же как и остальные модели машинного обучения, а именно:\n",
    "\n",
    "1) По данному входу (input) она должна выдавать предсказания (output)\n",
    "\n",
    "2) Она должна быть обучаемой (уметь подстраиваться под имеющиеся данные)\n",
    "\n",
    "Остановимся пока на первом пункте. Назовем метод, который по данному входу дает какие-то предсказания **forward** (если вы делали предыдущее домашнее задание - то это в точности метод **forward_pass** из него). Обратите внимание что метод **forward** должен не только возвращать значение output, но и сохранять его в поле self.output.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=img/black-box.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ноутбуке modules.ipynb вы найдете несколько различных классов:\n",
    "\n",
    "**Module** - этот самый описанный выше черный ящик. Пока что вас в нем интересует только метод **forward** который по данному input считает необходимый output. Все остальные модули отнаследованы от этого класса.\n",
    "\n",
    "**Sequential** - это класс-контейнер. Он состоит из списка различных модулей. Метод forward в нем последовательно прогоняет input через каждый модуль: сначала input подается в первый модуль, затем output первого модуля подается как input второго и так далее.\n",
    "\n",
    "**Sigmoid** - это активационная функция. Ее метод forward для каждого элемента матрицы input считает значение сигмоиды в нем. output имеет ту же размерность, что и input.\n",
    "\n",
    "**ReLU** - это активационная функция. Ее метод forward для каждого элемента матрицы input считает значение ReLU в нем. output имеет ту же размерность, что и input.\n",
    "\n",
    "**Linear** - это линейный слой (без активационной функции!). При инициализации он принимает пару чисел - свою размерность (n_out) и размерность предыдущего слоя (n_in), и инициализирует свои веса $W$ случайным образом. Обратите внимание, чтo размерность матрицы весов $W$ это (n_out, n_in), то есть $W_{ji}$ - это вес соединения между i-м нейроном предыдущего слоя и j-м нейроном текущего слоя. Его метод forward вычисляет значение матрицы $output$ по формуле $$output_{ij} = \\sum_n input_{in} W_{jn},$$ где i - индекс объекта, а j - индекс нейрона текущего слоя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dumb forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте метод dumb_forward в `Modules.ipynb` для модуля Linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%run modules.ipynb\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте ваш dumb_forward на искуственно сгенерированных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_linear():\n",
    "    np.random.seed(0)\n",
    "    return Linear(input_size, num_classes)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(2)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    \n",
    "    # y закодирован с помощью one hot encoding, \n",
    "    y = np.zeros((5,3))\n",
    "    y[np.arange(5), [0, 1, 2, 2, 1]] = 1\n",
    "    return X, y\n",
    "\n",
    "linear = init_linear()\n",
    "X, y = init_toy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your output:\n",
      "[[-1.78353582  7.99548235 -7.62463112]\n",
      " [-2.72893658 -5.05142268 -6.22715898]\n",
      " [-0.87700256  8.11758142 -1.57474381]\n",
      " [-2.09900077 -4.33473482  2.89594784]\n",
      " [ 1.7546528   2.23098844 -3.63738354]]\n",
      "\n",
      "correct output:\n",
      "[[ -1.54788446  -6.00658097   6.39369587]\n",
      " [ -3.07885716  -8.08969546  11.56531411]\n",
      " [  0.19697038  -9.59100515   5.43998673]\n",
      " [ -0.31239372  -5.33085678   1.94239605]\n",
      " [ -1.66825993   1.10786278   0.51528511]]\n",
      "\n",
      "Difference between your scores and correct scores:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "output = linear.dumb_forward(X)\n",
    "print('Your output:')\n",
    "print(output)\n",
    "print()\n",
    "print('correct output:')\n",
    "correct_output = np.asarray(\n",
    "[[ -1.54788446,  -6.00658097,   6.39369587],\n",
    " [ -3.07885716,  -8.08969546,  11.56531411],\n",
    " [  0.19697038,  -9.59100515,   5.43998673],\n",
    " [ -0.31239372,  -5.33085678,  1.94239605],\n",
    " [ -1.66825993,   1.10786278,   0.51528511]])\n",
    "print(correct_output)\n",
    "print()\n",
    "\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(rel_error(output, correct_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте метод forward в модуле Linear с помощью матричных операций библиотеки numpy и протестируйте его на тех же данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your output:\n",
      "[[-1.78353582  7.99548235 -7.62463112]\n",
      " [-2.72893658 -5.05142268 -6.22715898]\n",
      " [-0.87700256  8.11758142 -1.57474381]\n",
      " [-2.09900077 -4.33473482  2.89594784]\n",
      " [ 1.7546528   2.23098844 -3.63738354]]\n",
      "\n",
      "correct output:\n",
      "[[ -1.54788446  -6.00658097   6.39369587]\n",
      " [ -3.07885716  -8.08969546  11.56531411]\n",
      " [  0.19697038  -9.59100515   5.43998673]\n",
      " [ -0.31239372  -5.33085678   1.94239605]\n",
      " [ -1.66825993   1.10786278   0.51528511]]\n",
      "\n",
      "Difference between your scores and correct scores:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "%run modules.ipynb\n",
    "\n",
    "output = linear.forward(X)\n",
    "print('Your output:')\n",
    "print(output)\n",
    "print()\n",
    "print('correct output:')\n",
    "print(correct_output)\n",
    "print()\n",
    "\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(rel_error(output, correct_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посчитаем выход двухслойной сети, значение для которой мы считали на лекции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequential = Sequential()\n",
    "linear_small = Linear(n_in=2, n_out=2)\n",
    "linear_small.W = np.array([[0.9, 0.3], [0.2, 0.8]])\n",
    "sequential.add(linear_small)\n",
    "sequential.add(Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your output:\n",
      "[[ 0.7407749   0.64565631]]\n",
      "\n",
      "correct output:\n",
      "[[ 0.7408  0.6457]]\n",
      "\n",
      "Difference between your scores and correct scores:\n",
      "3.38355680721e-05\n"
     ]
    }
   ],
   "source": [
    "X_small = np.array([[1, 0.5]])\n",
    "output_small = sequential.forward(X_small)\n",
    "print('Your output:')\n",
    "print(output_small)\n",
    "print()\n",
    "print('correct output:')\n",
    "correct_output_small = np.asarray([[0.7408, 0.6457]])\n",
    "print(correct_output_small)\n",
    "print()\n",
    "\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(rel_error(output_small, correct_output_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полкчив какие-то предсказание от нейронной сети, мы хотим понять две вещи:\n",
    "\n",
    "1) Насколько она ошиблась\n",
    "\n",
    "2) Как надо поменять веса чтобы уменьшить это ошибку\n",
    "\n",
    "Для подсчета ошибки в modules.ipynb реализован еще один абстрактный класс - класс **Criterion**, черный ящик соответствующий функции потерь. У него есть метод forward, который принимает два аргумента: предсказанные значения и истинные значения, и возвращает значение функции потерь для данной пары. Также реализована среднеквадратичная функция потерь - **MSECriterion**.\n",
    "\n",
    "<img src='img/Criterion.png' width=500pt>\n",
    "\n",
    "Мы хотим обучаться методом градиентного спуска. Для этого нам нужно посчитать частные производные функции потерь по всем параметрам модели. Мы будем делать это методом обратного распространения ошибки, в основе которого лежит следующая идея: зная градиент функции потерь по выходу модуля мы легко можем посчитать градиент функции потерь по параметрам модуля и по его входу.\n",
    "\n",
    "<img src='img/Backprop.png' width=500>\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial input_i} = \\sum_n \\frac{\\partial L}{\\partial output_n}\\frac{\\partial output_n}{\\partial input_i}, \\frac{\\partial L}{\\partial parameter} = \\sum_n \\frac{\\partial L}{\\partial output_n}\\frac{\\partial output_n}{\\partial parameter}$$\n",
    "\n",
    "В частности для линейного слоя имеем формулы $$\\frac{\\partial L}{\\partial input_{ik}}= \\sum_n \\frac{\\partial L}{\\partial output_{in}}W_{nk},\\frac{\\partial L}{\\partial W_{nk}}= \\sum_i\\frac{\\partial L}{\\partial output_{in}}input_{k}$$\n",
    "\n",
    "где i - это индекс объекта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За подсчет градиентов функции ошибки по входу модуля и по параметрам модуля отвечают методы **accGradParameters** и **updateGradInput**, каждый из них на вход принимает input, использованный в методе forward ранее, и gradOutput, матрица производных функции потерь по каждому из выходов (имеет ту же размерность, что и output модуля). \n",
    "\n",
    "**accGradParameters** считает матрицу производных функции потерь по параметрам модуля и сохраняет ее.\n",
    "\n",
    "**updateGradInput** считает матрицу производных функции потерь по входу модуля и сохраняет ее в поле self.gradInput."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте методы updateGradInput и accGradParameters для модуля Linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your gradient w.r.t W:\n",
      "[[  27.44520232   30.63657384    6.58593022   -7.64214714]\n",
      " [  -5.08387273   14.22907449  -75.6155538   167.41204088]\n",
      " [  68.87172308    6.4708471    61.91460699  -47.26726932]]\n",
      "\n",
      "correct gradient w.r.t W:\n",
      "[[  27.44520202   30.63657417    6.58592967   -7.64214718]\n",
      " [  -5.08387288   14.22907374  -75.61555364  167.41204121]\n",
      " [  68.87172361    6.47084732   61.91460642  -47.26726957]]\n",
      "\n",
      "Difference between your scores and correct scores:\n",
      "4.15202955949e-08\n",
      "Your gradient w.r.t input:\n",
      "[[-1.71261934  0.58248877 -1.20374468  1.11486685]\n",
      " [-1.02340544 -0.29771075 -0.68774671 -1.06928023]\n",
      " [-0.74254431  0.51827758 -0.53915275  1.22659583]\n",
      " [ 0.44302281 -0.5220335   0.24317518 -0.69506347]\n",
      " [-0.67793944  0.3924578  -0.38305249  0.18236823]]\n",
      "\n",
      "correct gradient w.r.t input:\n",
      "[[-1.71261973  0.58248872 -1.20374466  1.11486713]\n",
      " [-1.02340536 -0.2977103  -0.68774639 -1.06928013]\n",
      " [-0.74254416  0.51827769 -0.53915272  1.226595  ]\n",
      " [ 0.4430234  -0.52203362  0.24317544 -0.69506214]\n",
      " [-0.67794019  0.39245762 -0.38305288  0.18236861]]\n",
      "\n",
      "Difference between your scores and correct scores:\n",
      "1.04813969181e-06\n"
     ]
    }
   ],
   "source": [
    "%run modules.ipynb\n",
    "\n",
    "# Используйте численное дифференцирования чтобы проверить вашу реализацию подсчета градиента\n",
    "# Если ваша реализация верна, то относительная ошибка будет не больше 1e-5\n",
    "\n",
    "linear = init_linear()\n",
    "output = linear.forward(X)\n",
    "\n",
    "mse = MSECriterion()\n",
    "grad_output = mse.updateGradInput(output, y)\n",
    "\n",
    "def calc_numerical_grad_for_linear(X=X, linear=linear, eps=1e-8):\n",
    "    n_objects, input_size = X.shape\n",
    "    n_classes, _ = linear.W.shape\n",
    "    W = linear.W.copy()\n",
    "    W_grad = np.zeros_like(W)\n",
    "    for row_idx in range(num_classes):\n",
    "        for column_idx in range(input_size):\n",
    "            linear.W = W.copy()\n",
    "            linear.W[row_idx][column_idx] += eps\n",
    "            right_output = mse.forward(linear.forward(X), y)\n",
    "            linear.W = W.copy()\n",
    "            linear.W[row_idx][column_idx] -= eps\n",
    "            left_output = mse.forward(linear.forward(X), y)\n",
    "            W_grad[row_idx][column_idx] = (right_output - left_output) / (2 * eps)\n",
    "    X_grad = np.zeros_like(X)\n",
    "    for obj_idx in range(n_objects):\n",
    "        for column_idx in range(input_size):\n",
    "            right_X = X.copy()\n",
    "            right_X[obj_idx][column_idx] += eps\n",
    "            right_output = mse.forward(linear.forward(right_X), y)\n",
    "            left_X = X.copy()\n",
    "            left_X[obj_idx][column_idx] -= eps\n",
    "            left_output = mse.forward(linear.forward(left_X), y)\n",
    "            X_grad[obj_idx][column_idx] = (right_output - left_output) / (2 * eps)\n",
    "    return W_grad, X_grad\n",
    "\n",
    "W_grad_numerical, X_grad_numerical = calc_numerical_grad_for_linear()\n",
    "X_grad = linear.backward(X, grad_output)\n",
    "W_grad = linear.getGradParameters()[0]\n",
    "\n",
    "print('Your gradient w.r.t W:')\n",
    "print(W_grad)\n",
    "print()\n",
    "print('correct gradient w.r.t W:')\n",
    "print(W_grad_numerical)\n",
    "print()\n",
    "\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(rel_error(W_grad, W_grad_numerical))\n",
    "print('Your gradient w.r.t input:')\n",
    "print(X_grad)\n",
    "print()\n",
    "print('correct gradient w.r.t input:')\n",
    "print(X_grad_numerical)\n",
    "print()\n",
    "\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(rel_error(X_grad, X_grad_numerical))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
